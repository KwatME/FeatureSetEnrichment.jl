{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ba47c-dcfd-4973-b472-200db23a7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85075f8d-75ba-4fb5-be0c-d56b798165ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Support\n",
    "using GCTGMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25618fe-79dc-49e0-bee4-6ac273975ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_set_to_element_ = Dict(\n",
    "    splitpath(path)[end] => read_gmt(path) for\n",
    "    path in read_directory(\"/Users/kwat/Desktop/gene_sets/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a66f52-9ab5-40d3-aab4-e3cbcc31f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using FeatureSetEnrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b75e0a-3c53-4fd9-aa33-71edd712f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_ = collect(keys(score_set_new([\"a\", \"b\"], [-1.0, 1.0], [\"a\"]; plot = false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f61ac-6359-49db-94c4-cfd07db70a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_directory_path = \"/Users/kwat/Desktop/benchmarks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bf6b6-cdf0-40e7-ae63-3cd930df355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a54b9-a623-45e6-a116-4db5fc8a25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_benchmark(benchmark_directory_path)\n",
    "\n",
    "    element_, score_ =\n",
    "        eachcol(read_data(joinpath(benchmark_directory_path, \"gene_x_score.tsv\")))\n",
    "\n",
    "    json_dict = read_json(joinpath(benchmark_directory_path, \"gene_sets.json\"))\n",
    "\n",
    "    set_to_element_ = Dict{String,Vector{String}}()\n",
    "\n",
    "    for gmt in json_dict[\"gene_sets_tested\"]\n",
    "\n",
    "        merge!(set_to_element_, name_to_set_to_element_[gmt])\n",
    "\n",
    "    end\n",
    "\n",
    "    return element_, score_, set_to_element_, json_dict[\"gene_sets_positive\"]\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9debf4a-396c-470a-9727-7cbcda788c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ded6a4-96d8-41d6-8956-ad289dd58193",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e11d2-e9f7-4eca-9952-298e9411b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_ = Vector{String}()\n",
    "\n",
    "n_gene_ = Vector{Int64}()\n",
    "\n",
    "benchmark_x_method_row_ = Vector{Vector{Float64}}()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebecdb-eacc-4289-ae18-86e8b462e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark_directory_path in read_directory(benchmarks_directory_path)\n",
    "\n",
    "    println(splitpath(benchmark_directory_path)[end])\n",
    "\n",
    "    element_, score_, set_to_element_, benchmark_set_ =\n",
    "        load_benchmark(benchmark_directory_path)\n",
    "\n",
    "    if compute\n",
    "\n",
    "        set_to_method_to_statistic = score_set_new(element_, score_, set_to_element_)\n",
    "\n",
    "        # TODO: try without collect\n",
    "        set_ = collect(keys(set_to_method_to_statistic))\n",
    "\n",
    "        set_x_method = convert_vector_of_vector_to_matrix([\n",
    "            collect(values(set_to_method_to_statistic[set_[set_i]])) for\n",
    "            set_i = 1:length(set_)\n",
    "        ])\n",
    "\n",
    "    end\n",
    "\n",
    "    for set in benchmark_set_\n",
    "\n",
    "        benchmark = string(splitpath(benchmark_directory_path)[end], '.', set)\n",
    "\n",
    "        push!(benchmark_, benchmark)\n",
    "\n",
    "        n_gene = length(set_to_element_[set])\n",
    "\n",
    "        push!(n_gene_, n_gene)\n",
    "\n",
    "        if compute\n",
    "\n",
    "            benchmark_x_method_row = Vector{Float64}()\n",
    "\n",
    "            # TODO: simplify loop\n",
    "            for (method_i, set_score_) in enumerate(eachcol(set_x_method))\n",
    "\n",
    "                is_negative_ = set_score_ .< 0\n",
    "\n",
    "                is_positive_ = 0 .<= set_score_\n",
    "\n",
    "                set_negative_ = set_[is_negative_]\n",
    "\n",
    "                set_positive_ = set_[is_positive_]\n",
    "\n",
    "                negative_ =\n",
    "                    .-normalize(.-set_score_[is_negative_], \"1224\") / sum(is_negative_)\n",
    "\n",
    "                positive_ = normalize(set_score_[is_positive_], \"1224\") / sum(is_positive_)\n",
    "\n",
    "                i_negative = findfirst(set_negative_ .== set)\n",
    "\n",
    "                i_positive = findfirst(set_positive_ .== set)\n",
    "\n",
    "                if i_negative != nothing\n",
    "\n",
    "                    benchmark_score = negative_[i_negative]\n",
    "\n",
    "                elseif i_positive != nothing\n",
    "\n",
    "                    benchmark_score = positive_[i_positive]\n",
    "\n",
    "                else\n",
    "\n",
    "                    error(method_[method_i], \" returned \", benchmark_score)\n",
    "\n",
    "                end\n",
    "\n",
    "                push!(benchmark_x_method_row, benchmark_score)\n",
    "\n",
    "            end\n",
    "\n",
    "            push!(benchmark_x_method_row_, benchmark_x_method_row)\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "n_benchmark = length(benchmark_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b91a8-5f1c-4d42-9508-c3c1c3fcc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3e754-4b5f-45d5-ba45-f6668b256916",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_x_method_path = \"benchmark_x_method.tsv\"\n",
    "\n",
    "axis0_name = \"Benchmark\"\n",
    "\n",
    "if compute\n",
    "\n",
    "    df = DataFrame(convert_vector_of_vector_to_matrix(benchmark_x_method_row_), method_)\n",
    "\n",
    "    insertcols!(df, 1, axis0_name => benchmark_)\n",
    "\n",
    "    CSV.write(benchmark_x_method_path, df)\n",
    "\n",
    "end\n",
    "\n",
    "benchmark_x_method = read_data(benchmark_x_method_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7a683-51c0-4627-9d34-d97b4b5dffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method_ = [\"is ks < area\", \"is ks < extreme\", \"a idrsw <> area\", \"a idrdw <> area\"]\n",
    "\n",
    "benchmark_x_method = Matrix(benchmark_x_method[!, method_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85767b46-18ab-4f22-b6d5-606e32d0d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "function peek_benchmark(id::String)::Nothing\n",
    "\n",
    "    benchmark, set = split(id, '.'; limit = 2)\n",
    "\n",
    "    element_, score_, set_to_element_, benchmark_set_ =\n",
    "        load_benchmark(joinpath(benchmarks_directory_path, benchmark))\n",
    "\n",
    "    score_set_new(element_, score_, set_to_element_[set])\n",
    "\n",
    "    return nothing\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1e22d-36c3-40ea-bc53-bd60bba4a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_x_method_metric = Matrix{Float64}(undef, size(benchmark_x_method))\n",
    "\n",
    "for benchmark_i = 1:length(benchmark_)\n",
    "\n",
    "    base_score = benchmark_x_method[benchmark_i, 1]\n",
    "\n",
    "    score_ = benchmark_x_method[benchmark_i, :]\n",
    "\n",
    "    if base_score < 0\n",
    "\n",
    "        peek_benchmark(benchmark_[benchmark_i])\n",
    "\n",
    "        metric_ = score_ .< base_score\n",
    "\n",
    "    else\n",
    "\n",
    "        metric_ = base_score .< score_\n",
    "\n",
    "    end\n",
    "\n",
    "    benchmark_x_method_metric[benchmark_i, :] = metric_\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391b22b-d86b-429e-92f6-7242b0838c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_sort_print(m::Matrix)::Nothing\n",
    "\n",
    "    for (n_better, method) in\n",
    "        zip(sort_like(Float64.(sum(eachrow(m))), method_; r = true)...)\n",
    "\n",
    "        println(Int64(n_better), \"    \", method)\n",
    "\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb76ce1-3387-432c-b6ca-c4a65ae44659",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sort_print(benchmark_x_method_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f16371-b2cf-49e2-94cd-3b1f82748abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7746f1-b663-471b-bd93-f290b1cd3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_style!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401e798-8737-44d9-8413-c2e660aef8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1319a2d-e0b1-47ab-916e-1cab04ee5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = benchmark_x_method[:, 1]\n",
    "\n",
    "for method_i = 2:length(method_)\n",
    "\n",
    "    display(\n",
    "        plot_x_y(\n",
    "            [x, x],\n",
    "            [x, benchmark_x_method[:, method_i]];\n",
    "            text_ = [benchmark_, benchmark_],\n",
    "            mode_ = [\"line\", \"markers\"],\n",
    "            layout = Layout(\n",
    "                xaxis_title_text = method_[1],\n",
    "                yaxis_title_text = method_[method_i],\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
