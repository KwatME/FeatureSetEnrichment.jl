{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cde750-8ccd-434f-82f8-94e84d6a6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f0dc7-21c5-4531-a5c2-6790849cf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using JSON\n",
    "\n",
    "using DataIO\n",
    "using GCTGMT\n",
    "using Normalization\n",
    "using Support\n",
    "\n",
    "using FeatureSetEnrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25618fe-79dc-49e0-bee4-6ac273975ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_set_to_element_ = Dict(\n",
    "    splitpath(path)[end] => read_gmt(path) for\n",
    "    path in read_directory(\"/Users/kwat/Downloads/gene_sets/\")\n",
    ")\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b75e0a-3c53-4fd9-aa33-71edd712f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_ = Vector{String}()\n",
    "\n",
    "for method in keys(score_set_new([\"a\", \"b\"], [-1.0, 1.0], [\"a\"]; plot = false))\n",
    "\n",
    "    push!(method_, string(method, \" extreme\"), string(method, \" area\"))\n",
    "\n",
    "end\n",
    "\n",
    "n_method = length(method_)\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f61ac-6359-49db-94c4-cfd07db70a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_directory_path = \"/Users/kwat/Downloads/benchmarks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f54201-7b3d-4edf-93cd-ce9b5415b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_ = Vector{String}()\n",
    "\n",
    "benchmark_size_ = Vector{Int64}()\n",
    "\n",
    "compute = false\n",
    "\n",
    "benchmark_x_method_rows = Vector{Vector{Float64}}()\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc898b0f-e4bc-4a4e-ba7c-facd6d37730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark_directory_path in read_directory(benchmarks_directory_path)\n",
    "\n",
    "    name = splitpath(benchmark_directory_path)[end]\n",
    "\n",
    "    println(name)\n",
    "\n",
    "    element_, score_ =\n",
    "        eachcol(read_data(joinpath(benchmark_directory_path, \"gene_x_score.tsv\")))\n",
    "\n",
    "    json_dict = JSON.parse(open(joinpath(benchmark_directory_path, \"gene_sets.json\")))\n",
    "\n",
    "    set_to_element_ = Dict{String,Vector{String}}()\n",
    "\n",
    "    for gmt_path in json_dict[\"gene_sets_tested\"]\n",
    "\n",
    "        merge!(set_to_element_, name_to_set_to_element_[splitpath(gmt_path)[end]])\n",
    "\n",
    "    end\n",
    "\n",
    "    set_ = sort(collect(keys(set_to_element_)))\n",
    "\n",
    "    n_set = length(set_)\n",
    "\n",
    "    println(\"    \", n_set, \" set.\")\n",
    "\n",
    "    if compute\n",
    "\n",
    "        set_to_method_to_result = sort(score_set_new(element_, score_, set_to_element_))\n",
    "\n",
    "        set_x_method = Matrix{Float64}(undef, n_set, n_method)\n",
    "\n",
    "        for (set_i, (set, method_to_result)) in enumerate(set_to_method_to_result)\n",
    "\n",
    "            set_x_method_row = []\n",
    "\n",
    "            for result in values(method_to_result)\n",
    "\n",
    "                append!(set_x_method_row, result[2:3])\n",
    "\n",
    "            end\n",
    "\n",
    "            set_x_method[set_i, :] = set_x_method_row\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "    for set in json_dict[\"gene_sets_positive\"]\n",
    "\n",
    "        print(\"    \")\n",
    "\n",
    "        if !in(set, set_)\n",
    "\n",
    "            println(set, \" (missing)\")\n",
    "\n",
    "            continue\n",
    "\n",
    "        end\n",
    "\n",
    "        benchmark = string(name, '.', set)\n",
    "\n",
    "        benchmark_size = length(set_to_element_[set])\n",
    "\n",
    "        println(set, \" (\", benchmark_size, \")\")\n",
    "\n",
    "        push!(benchmark_, benchmark)\n",
    "\n",
    "        push!(benchmark_size_, benchmark_size)\n",
    "\n",
    "        if compute\n",
    "\n",
    "            benchmark_x_method_row = Vector{Float64}()\n",
    "\n",
    "            for (method_i, set_score_) in enumerate(eachcol(set_x_method))\n",
    "\n",
    "                is_negative_ = set_score_ .< 0\n",
    "\n",
    "                is_positive_ = 0 .<= set_score_\n",
    "\n",
    "                set_negative_ = set_[is_negative_]\n",
    "\n",
    "                set_positive_ = set_[is_positive_]\n",
    "\n",
    "                negative_ =\n",
    "                    .-normalize(.-set_score_[is_negative_], \"1224\") / sum(is_negative_)\n",
    "\n",
    "                positive_ = normalize(set_score_[is_positive_], \"1224\") / sum(is_positive_)\n",
    "\n",
    "                i_negative = findfirst(set_negative_ .== set)\n",
    "\n",
    "                i_positive = findfirst(set_positive_ .== set)\n",
    "\n",
    "                if i_negative != nothing\n",
    "\n",
    "                    benchmark_score = negative_[i_negative]\n",
    "\n",
    "                elseif i_positive != nothing\n",
    "\n",
    "                    benchmark_score = positive_[i_positive]\n",
    "\n",
    "                else\n",
    "\n",
    "                    benchmark_score = set_score_[findfirst(set_ .== set)]\n",
    "\n",
    "                    println(\"        \", method_[method_i], \" returned \", benchmark_score)\n",
    "\n",
    "                    #TODO: check all NaN row later\n",
    "\n",
    "                end\n",
    "\n",
    "                push!(benchmark_x_method_row, benchmark_score)\n",
    "\n",
    "            end\n",
    "\n",
    "            push!(benchmark_x_method_rows, benchmark_x_method_row)\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace5c1a-bcc2-48d7-90fb-b2e56c28071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_x_method_path = \"benchmark_x_method.tsv\"\n",
    "\n",
    "if compute\n",
    "\n",
    "    benchmark_x_method = convert_vector_of_vector_to_matrix(benchmark_x_method_rows)\n",
    "\n",
    "    benchmark_x_method = DataFrame(benchmark_x_method, method_)\n",
    "\n",
    "    insertcols!(benchmark_x_method, 1, \"Benchmark\" => benchmark_)\n",
    "\n",
    "    CSV.write(benchmark_x_method_path, benchmark_x_method)\n",
    "\n",
    "end\n",
    "\n",
    "benchmark_x_method = read_data(benchmark_x_method_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1e22d-36c3-40ea-bc53-bd60bba4a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_x_method_metric = Matrix{Int64}(undef, length(benchmark_), n_method)\n",
    "\n",
    "for (benchmark_i, benchmark_score_) in\n",
    "    enumerate(eachrow(select(benchmark_x_method, Not(\"Benchmark\"))))\n",
    "\n",
    "    classic_score = benchmark_score_[\"Is KS < extreme\"]\n",
    "\n",
    "    benchmark_score_ = Vector(benchmark_score_)\n",
    "\n",
    "    if classic_score < 0\n",
    "\n",
    "        metric_ = benchmark_score_ .< classic_score\n",
    "\n",
    "    else\n",
    "\n",
    "        metric_ = classic_score .< benchmark_score_\n",
    "\n",
    "    end\n",
    "\n",
    "    benchmark_x_method_metric[benchmark_i, :] = metric_\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391b22b-d86b-429e-92f6-7242b0838c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_sort(m::Matrix)::Nothing\n",
    "\n",
    "    for (method, n_better) in zip(sort_like(sum(eachrow(m)), method_)...)\n",
    "\n",
    "        println(method, \"    \", n_better)\n",
    "\n",
    "    end\n",
    "\n",
    "    return nothing\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef396782-c5ce-408f-9153-8859fe40dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = \"=\"^8\n",
    "\n",
    "benchmark_size_maximum = maximum(benchmark_size_)\n",
    "\n",
    "println(buffer, \" < \", benchmark_size_maximum, \" \", buffer)\n",
    "\n",
    "sum_sort(benchmark_x_method_metric)\n",
    "\n",
    "increment = 50\n",
    "\n",
    "for (minimum_size, maximum_size) in [\n",
    "    ((i - 1) * increment, i * increment) for\n",
    "    i = 1:Int64(ceil(benchmark_size_maximum / increment))\n",
    "]\n",
    "\n",
    "    println(buffer, \" \", minimum_size, \" < \", maximum_size, \" \", buffer)\n",
    "\n",
    "    is_selected = minimum_size .< benchmark_size_ .< maximum_size\n",
    "\n",
    "    n_selected = sum(is_selected)\n",
    "\n",
    "    println(n_selected)\n",
    "\n",
    "    if 1 < n_selected\n",
    "\n",
    "        sum_sort(benchmark_x_method_metric[is_selected, :])\n",
    "\n",
    "    end\n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
