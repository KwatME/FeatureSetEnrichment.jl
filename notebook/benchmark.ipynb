{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ba47c-dcfd-4973-b472-200db23a7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85075f8d-75ba-4fb5-be0c-d56b798165ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Support\n",
    "\n",
    "using GCTGMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25618fe-79dc-49e0-bee4-6ac273975ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_set_to_element_ = Dict(\n",
    "    splitpath(path)[end] => read_gmt(path) for\n",
    "    path in read_directory(\"/Users/kwat/Desktop/gene_sets/\")\n",
    ")\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a66f52-9ab5-40d3-aab4-e3cbcc31f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using FeatureSetEnrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b75e0a-3c53-4fd9-aa33-71edd712f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_ = Vector{String}()\n",
    "\n",
    "for method in keys(score_set_new([\"a\", \"b\"], [-1.0, 1.0], [\"a\"]; plot = false))\n",
    "\n",
    "    push!(method_, string(method, \" extreme\"), string(method, \" area\"))\n",
    "\n",
    "end\n",
    "\n",
    "n_method = length(method_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f61ac-6359-49db-94c4-cfd07db70a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_directory_path = \"/Users/kwat/Desktop/benchmarks2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bf6b6-cdf0-40e7-ae63-3cd930df355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a54b9-a623-45e6-a116-4db5fc8a25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_benchmark(benchmark_directory_path)\n",
    "\n",
    "    benchmark = splitpath(benchmark_directory_path)[end]\n",
    "\n",
    "    element_, score_ =\n",
    "        eachcol(read_data(joinpath(benchmark_directory_path, \"gene_x_score.tsv\")))\n",
    "\n",
    "    json_dict = read_json(joinpath(benchmark_directory_path, \"gene_sets.json\"))\n",
    "\n",
    "    set_to_element_ = Dict{String,Vector{String}}()\n",
    "\n",
    "    for gmt in json_dict[\"gene_sets_tested\"]\n",
    "\n",
    "        merge!(set_to_element_, name_to_set_to_element_[gmt])\n",
    "\n",
    "    end\n",
    "\n",
    "    set_ = sort(collect(keys(set_to_element_)))\n",
    "\n",
    "    n_set = length(set_)\n",
    "\n",
    "    println(\"Benchmark \", benchmark, \" (\", n_set, \" set)\")\n",
    "\n",
    "    return element_, score_, set_to_element_, json_dict[\"gene_sets_positive\"]\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9debf4a-396c-470a-9727-7cbcda788c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute = true\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ded6a4-96d8-41d6-8956-ad289dd58193",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49c782-b04b-4da7-a5d0-af7fe1689803",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_ = Vector{String}()\n",
    "\n",
    "benchmark_size_ = Vector{Int64}()\n",
    "\n",
    "benchmark_x_method_row_ = Vector{Vector{Float64}}()\n",
    "\n",
    "for benchmark_directory_path in read_directory(benchmarks_directory_path)\n",
    "\n",
    "    element_, score_, set_to_element_, benchmark_set_ =\n",
    "        load_benchmark(benchmark_directory_path)\n",
    "\n",
    "    set_ = sort(collect(keys(set_to_element_)))\n",
    "\n",
    "    if compute\n",
    "\n",
    "        set_to_method_to_result = sort(score_set_new(element_, score_, set_to_element_))\n",
    "\n",
    "        set_x_method = Matrix{Float64}(undef, length(set_), n_method)\n",
    "\n",
    "        for (set_i, (set, method_to_result)) in enumerate(set_to_method_to_result)\n",
    "\n",
    "            set_x_method_row = []\n",
    "\n",
    "            for result in values(method_to_result)\n",
    "\n",
    "                append!(set_x_method_row, result[2:3])\n",
    "\n",
    "            end\n",
    "\n",
    "            set_x_method[set_i, :] = set_x_method_row\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "    for set in benchmark_set_\n",
    "\n",
    "        benchmark = string(splitpath(benchmark_directory_path)[end], '.', set)\n",
    "\n",
    "        benchmark_size = length(set_to_element_[set])\n",
    "\n",
    "        #println(\"    \", set, \" (\", benchmark_size, \")\")\n",
    "\n",
    "        push!(benchmark_, benchmark)\n",
    "\n",
    "        push!(benchmark_size_, benchmark_size)\n",
    "\n",
    "        if compute\n",
    "\n",
    "            benchmark_x_method_row = Vector{Float64}()\n",
    "\n",
    "            for (method_i, set_score_) in enumerate(eachcol(set_x_method))\n",
    "\n",
    "                is_negative_ = set_score_ .< 0\n",
    "\n",
    "                is_positive_ = 0 .<= set_score_\n",
    "\n",
    "                set_negative_ = set_[is_negative_]\n",
    "\n",
    "                set_positive_ = set_[is_positive_]\n",
    "\n",
    "                negative_ =\n",
    "                    .-normalize(.-set_score_[is_negative_], \"1224\") / sum(is_negative_)\n",
    "\n",
    "                positive_ = normalize(set_score_[is_positive_], \"1224\") / sum(is_positive_)\n",
    "\n",
    "                i_negative = findfirst(set_negative_ .== set)\n",
    "\n",
    "                i_positive = findfirst(set_positive_ .== set)\n",
    "\n",
    "                if i_negative != nothing\n",
    "\n",
    "                    benchmark_score = negative_[i_negative]\n",
    "\n",
    "                elseif i_positive != nothing\n",
    "\n",
    "                    benchmark_score = positive_[i_positive]\n",
    "\n",
    "                else\n",
    "\n",
    "                    error(method_[method_i], \" returned \", benchmark_score)\n",
    "\n",
    "                end\n",
    "\n",
    "                push!(benchmark_x_method_row, benchmark_score)\n",
    "\n",
    "            end\n",
    "\n",
    "            push!(benchmark_x_method_row_, benchmark_x_method_row)\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "n_benchmark = length(benchmark_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff109999-ac77-46cf-911b-19149be893ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4e831-eff7-40ae-99a6-229abb8a2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_x_method_path = \"benchmark_x_method.tsv\"\n",
    "\n",
    "row_name = \"Benchmark\"\n",
    "\n",
    "if compute\n",
    "\n",
    "    df = DataFrame(convert_vector_of_vector_to_matrix(benchmark_x_method_row_), method_)\n",
    "\n",
    "    insertcols!(df, 1, row_name => benchmark_)\n",
    "\n",
    "    CSV.write(benchmark_x_method_path, df)\n",
    "\n",
    "end\n",
    "\n",
    "benchmark_x_method = read_data(benchmark_x_method_path)\n",
    "\n",
    "method_ = [\"is ks < area\", \"is ks < extreme\", \"a idrsw <> area\", \"a idrdw <> area\"]\n",
    "\n",
    "benchmark_x_method = Matrix(benchmark_x_method[!, method_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb56a0-3453-419e-96e8-8da4ae83969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = \"is ks < area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85767b46-18ab-4f22-b6d5-606e32d0d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "function peek_benchmark(id::String)::Nothing\n",
    "\n",
    "    benchmark, set = split(id, '.'; limit = 2)\n",
    "\n",
    "    element_, score_, set_to_element_, benchmark_set_ =\n",
    "        load_benchmark(joinpath(benchmarks_directory_path, benchmark))\n",
    "\n",
    "    score_set_new(element_, score_, set_to_element_[set])\n",
    "\n",
    "    return nothing\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1e22d-36c3-40ea-bc53-bd60bba4a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_x_method_metric = Matrix{Float64}(undef, size(benchmark_x_method))\n",
    "\n",
    "for benchmark_i = 1:n_benchmark\n",
    "\n",
    "    base_score = benchmark_x_method[benchmark_i, 1]\n",
    "\n",
    "    score_ = benchmark_x_method[benchmark_i, :]\n",
    "\n",
    "    if base_score < 0\n",
    "\n",
    "        #peek_benchmark(benchmark_[benchmark_i])\n",
    "\n",
    "        metric_ = score_ .< base_score\n",
    "\n",
    "    else\n",
    "\n",
    "        metric_ = base_score .< score_\n",
    "\n",
    "    end\n",
    "\n",
    "    benchmark_x_method_metric[benchmark_i, :] = metric_\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391b22b-d86b-429e-92f6-7242b0838c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_sort_print(m::Matrix)::Nothing\n",
    "\n",
    "    for (n_better, method) in\n",
    "        zip(sort_like(Float64.(sum(eachrow(m))), method_; r = true)...)\n",
    "\n",
    "        println(Int64(n_better), \"    \", method)\n",
    "\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77232f44-c22a-4fef-86ed-0fb7b85c2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "function print_header(minimum_size::Int64, maximum_size::Int64, n::Int64)::Nothing\n",
    "\n",
    "    buffer = \"=\"^8\n",
    "\n",
    "    println(\n",
    "        buffer,\n",
    "        \" Size range: \",\n",
    "        minimum_size,\n",
    "        \" < \",\n",
    "        maximum_size,\n",
    "        \" (\",\n",
    "        n,\n",
    "        \") \",\n",
    "        buffer,\n",
    "    )\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb76ce1-3387-432c-b6ca-c4a65ae44659",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_size_maximum = maximum(benchmark_size_)\n",
    "\n",
    "print_header(0, benchmark_size_maximum, n_benchmark)\n",
    "\n",
    "sum_sort_print(benchmark_x_method_metric)\n",
    "\n",
    "increment = 100\n",
    "\n",
    "for (minimum_size, maximum_size) in [\n",
    "    ((i - 1) * increment, i * increment) for\n",
    "    i = 1:Int64(ceil(benchmark_size_maximum / increment))\n",
    "]\n",
    "\n",
    "    is_selected = minimum_size .< benchmark_size_ .< maximum_size\n",
    "\n",
    "    print_header(minimum_size, maximum_size, sum(is_selected))\n",
    "\n",
    "    sum_sort_print(benchmark_x_method_metric[is_selected, :])\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f16371-b2cf-49e2-94cd-3b1f82748abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7746f1-b663-471b-bd93-f290b1cd3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_style!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401e798-8737-44d9-8413-c2e660aef8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1319a2d-e0b1-47ab-916e-1cab04ee5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = benchmark_x_method[:, 1]\n",
    "\n",
    "for method_i = 2:length(method_)\n",
    "\n",
    "    display(\n",
    "        plot_x_y(\n",
    "            [x, x],\n",
    "            [x, benchmark_x_method[:, method_i]];\n",
    "            text_ = [benchmark_, benchmark_],\n",
    "            mode_ = [\"line\", \"markers\"],\n",
    "            layout = Layout(\n",
    "                xaxis_title_text = base_name,\n",
    "                yaxis_title_text = method_[method_i],\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022be270-45f9-4d1f-89d1-67f239676977",
   "metadata": {},
   "outputs": [],
   "source": [
    "peek_benchmark(\"18.REACTOME_TRANSCRIPTIONAL_ACTIVITY_OF_SMAD2_SMAD3_SMAD4_HETEROTRIMER\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
